{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionDetection.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Qw9gyY6Aairr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "81055db0-b89f-49b1-d718-34721780b055",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527127765551,
          "user_tz": 240,
          "elapsed": 33594,
          "user": {
            "displayName": "Umar Ahsan",
            "photoUrl": "//lh3.googleusercontent.com/-KZJFBGO1WUE/AAAAAAAAAAI/AAAAAAAAAFk/G7WqmF0NCrk/s50-c-k-no/photo.jpg",
            "userId": "101835716642345366972"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The purpose of this part is to upload training file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Since we are using google colab so the above commands are just to upload the file to google drive for further processing\n",
        "# You can skip this part if you are are using your own local GPU/CPU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e27650b8-4a2f-42eb-b6bc-d3bfcf684e68\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e27650b8-4a2f-42eb-b6bc-d3bfcf684e68\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ISEAR.csv to ISEAR.csv\n",
            "User uploaded file \"ISEAR.csv\" with length 914779 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xAZgdTmf_zV_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3865
        },
        "outputId": "d8757d7a-460f-4adf-df92-a36f3ba09282",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527127865936,
          "user_tz": 240,
          "elapsed": 89251,
          "user": {
            "displayName": "Umar Ahsan",
            "photoUrl": "//lh3.googleusercontent.com/-KZJFBGO1WUE/AAAAAAAAAAI/AAAAAAAAAFk/G7WqmF0NCrk/s50-c-k-no/photo.jpg",
            "userId": "101835716642345366972"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing all the required NLTK libraries for further usage\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /content/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /content/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /content/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /content/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /content/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /content/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /content/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /content/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /content/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /content/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /content/nltk_data...\n",
            "       | Downloading package conll2000 to /content/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /content/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /content/nltk_data...\n",
            "       | Downloading package crubadan to /content/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /content/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /content/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /content/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /content/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /content/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /content/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /content/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /content/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /content/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /content/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /content/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /content/nltk_data...\n",
            "       | Downloading package kimmo to /content/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /content/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /content/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /content/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /content/nltk_data...\n",
            "       | Downloading package masc_tagged to /content/nltk_data...\n",
            "       | Downloading package moses_sample to /content/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /content/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /content/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /content/nltk_data...\n",
            "       | Downloading package nps_chat to /content/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /content/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /content/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /content/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /content/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /content/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /content/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /content/nltk_data...\n",
            "       | Downloading package ptb to /content/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /content/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /content/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /content/nltk_data...\n",
            "       | Downloading package rte to /content/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /content/nltk_data...\n",
            "       | Downloading package senseval to /content/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /content/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /content/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /content/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /content/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /content/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /content/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /content/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /content/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /content/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /content/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /content/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /content/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /content/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /content/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /content/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /content/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /content/nltk_data...\n",
            "       | Downloading package verbnet to /content/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package webtext to /content/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /content/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /content/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /content/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /content/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /content/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /content/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /content/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /content/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /content/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /content/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /content/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /content/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /content/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /content/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /content/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /content/nltk_data...\n",
            "       | Downloading package mte_teip5 to /content/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package perluniprops to /content/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /content/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /content/nltk_data...\n",
            "       | Downloading package porter_test to /content/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /content/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "_cUff7HGzhqh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Set: We are using ISEAR Dataset (An open source) for emotion detection\n",
        "\n",
        "\n",
        "The ISEAR dataset has been developped by the Swiss National Center of Competence in Research.\n",
        "\n",
        "The dataset contains 7 different emotions namely \"Anger\", \"Disgust\", \"Fear\", \"Guilt\", \"Joy\", \"Sadness\", \"Shame\". The emotions are taged based on the content present within a sentence. Each sentence is tagged with either one of the 7 emotions. There are 7570 sentences present within dataset.\n",
        "\n",
        "The ISEAR dataset is balanced as apporximately all of the emotions are repeating same number of times within the data."
      ]
    },
    {
      "metadata": {
        "id": "KxxfhUMU2xOe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The purpose of this module is to import the csv data and store it into the list format for processing."
      ]
    },
    {
      "metadata": {
        "id": "-984ZfDab9S4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8530a2cd-a2d5-4bab-95cf-870bdd7f2bed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527127881619,
          "user_tz": 240,
          "elapsed": 7926,
          "user": {
            "displayName": "Umar Ahsan",
            "photoUrl": "//lh3.googleusercontent.com/-KZJFBGO1WUE/AAAAAAAAAAI/AAAAAAAAAFk/G7WqmF0NCrk/s50-c-k-no/photo.jpg",
            "userId": "101835716642345366972"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Importing the required libraries\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from sklearn.cross_validation import train_test_split # This library is used for splitting the data for training and testing\n",
        "import io\n",
        "import string\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "df = pd.read_csv('ISEAR.csv') # Reading the dataframe\n",
        "\n",
        "a = pd.Series(df['joy'])\n",
        "b = pd.Series(df[' On days when I feel close to my partner and other friends  When I feel at peace with myself and also experience a close contact with people whom I regard greatly'])\n",
        "\n",
        "new_df = pd.DataFrame({'Text': b, 'Emotion': a})\n",
        "\n",
        "em_list = []\n",
        "text_list = []\n",
        "\n",
        "## create the training set\n",
        "train = []\n",
        "xtrain=[]\n",
        "xtest=[]\n",
        "exclude = set(string.punctuation)  ## stores all the punctuations\n",
        "lemma = WordNetLemmatizer() # used for lemmatization\n",
        "stop = set(stopwords.words('english'))\n",
        "negative=['']\n",
        "\n",
        "# The clean function is used to remove stop words, punctuations, perform lemmatization and remove any negative words if desired\n",
        "def clean(doc):\n",
        "  stop_free = \" \".join([i for i in doc.lower().split() if i not in stop if i not in negative])\n",
        "  punc_free = \"\".join([ch for ch in stop_free if ch not in exclude])\n",
        "  normalized = \" \".join([lemma.lemmatize(word) for word in punc_free.split()])\n",
        "  without_digits = \" \".join([x for x in normalized.split() if not (x.isdigit() \n",
        "                                          or x[0] == '-' and x[1:].isdigit())])\n",
        "  return without_digits\n",
        "# Defining all the categories\n",
        "all_categories = ['anger','disgust','fear','guilt','joy','sadness','shame']\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "  new_df.loc[i]['Text'] = clean(new_df.loc[i]['Text'])\n",
        "\n",
        "for i in range(new_df.shape[0]):\n",
        "  text_list.append(new_df.loc[i]['Text'])\n",
        "  em_list.append(new_df.loc[i]['Emotion'])\n",
        "  train.append([text_list[i], em_list[i]])\n",
        "            #print(self.train[0][1])\n",
        "\n",
        "xtrain, xtest = train_test_split(train, test_size=0.2) # Splitting the data set into 80% Training and 20% Testing\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWxgMRdG3gWH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding\n",
        "\n",
        "All of the unique words present within each sentence is encoded using OneHotEncoder"
      ]
    },
    {
      "metadata": {
        "id": "kZhL_02c9xuf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas  \n",
        "import numpy as np\n",
        "\n",
        "grandpair=[]\n",
        "\n",
        "# This for loop is used to get unique words within the data set for lavel encoding and then one hot encoding respectively\n",
        "for i in range(0,len(train)):\n",
        "  pairs= train[i][0].split()\n",
        "  grandpair = grandpair + pairs\n",
        "  \n",
        "unique_pair = list(set(grandpair)) \n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(unique_pair)\n",
        "feature = label_encoder.transform(unique_pair)\n",
        "\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False,n_values=len(unique_pair))\n",
        "\n",
        "# Now we are summing up all the one hot encoding for each sentence so that we can incorporate variable length sentence...\n",
        "# The shape of each row is approx 74700 columns which is the size of dictionary \n",
        "\n",
        "xdata_onehot = np.zeros((len(train),len(unique_pair)))\n",
        "xdata_encoded = np.zeros((len(train),100))\n",
        "\n",
        "for j in range (0,len(train)):\n",
        "  sentence = label_encoder.transform(train[j][0].split())\n",
        "  for k in range(0,len(sentence)):\n",
        "    xdata_encoded[j][k]= sentence[k]\n",
        "  sentence = sentence.reshape(len(sentence), 1)\n",
        "  encoded_features = onehot_encoder.fit_transform(sentence)\n",
        "\n",
        "  xdata_onehot_temp= np.zeros((1,len(unique_pair)))\n",
        "  for k in range (0,len(encoded_features)):\n",
        "    xdata_onehot_temp = np.add(xdata_onehot_temp,encoded_features[k,:])\n",
        "  xdata_onehot[j]= xdata_onehot_temp\n",
        "\n",
        "\n",
        "# Now all the emotions are also one hot encoded  \n",
        "emotion_encoder1 = LabelEncoder()\n",
        "emotionpair=[]\n",
        "number_of_emotions=7\n",
        "for i in range(0,len(train)):\n",
        "  pairs= train[i][1].split()\n",
        "  emotionpair = emotionpair + pairs \n",
        "  \n",
        "emotion_encoder1.fit(emotionpair)\n",
        "emotion_label = np.zeros((len(train),1))\n",
        "\n",
        "onehot_emotion_enc = OneHotEncoder(sparse=False,n_values=7)\n",
        "\n",
        "\n",
        "for j in range (0,len(train)):\n",
        "  sentence = emotion_encoder1.transform(train[j][1].split())\n",
        "  sentence = sentence.reshape(len(sentence), 1)\n",
        "  emotion_label[j]= sentence\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o10M2w7v5Cc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Previously, we are performing one hot encoding on all of the data set. Now, we are seprately encoding the training and test data set for further analyzing our model."
      ]
    },
    {
      "metadata": {
        "id": "X1KmBGm3Wb8p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Train and test data is separately encoded\n",
        "traindata_encoded = np.zeros((len(xtrain),100))\n",
        "testdata_encoded = np.zeros((len(xtest),100))\n",
        "trainlabel_encoded = np.zeros((len(xtrain),1))\n",
        "testlabel_encoded = np.zeros((len(xtest),1))\n",
        "\n",
        "for j in range (0,len(xtrain)):\n",
        "  sentence = label_encoder.transform(xtrain[j][0].split())\n",
        "  for k in range(0,len(sentence)):\n",
        "    traindata_encoded[j][k]= sentence[k]\n",
        "    \n",
        "for j in range (0,len(xtest)):\n",
        "  sentence = label_encoder.transform(xtest[j][0].split())\n",
        "  for k in range(0,len(sentence)):\n",
        "    testdata_encoded[j][k]= sentence[k]\n",
        "\n",
        "    \n",
        "for j in range (0,len(xtrain)):\n",
        "  sentence = emotion_encoder1.transform(xtrain[j][1].split())\n",
        "  sentence = sentence.reshape(len(sentence), 1)\n",
        "  trainlabel_encoded[j]= sentence\n",
        "  \n",
        "  \n",
        "for j in range (0,len(xtest)):\n",
        "  sentence = emotion_encoder1.transform(xtest[j][1].split())\n",
        "  sentence = sentence.reshape(len(sentence), 1)\n",
        "  testlabel_encoded[j]= sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xV3oR6D95epS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "## LSTM Cells - Keras\n",
        "\n",
        "Learning Rate: 0.006, \n",
        "Optimizer: Adam, \n",
        "Layers: 1 LSTM cell with 140 neurons, \n",
        "Dropout: 0.2"
      ]
    },
    {
      "metadata": {
        "id": "bP65g-5xPaAt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1181
        },
        "outputId": "a0a9b597-a7aa-4568-a050-65efaad9b170",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527128321525,
          "user_tz": 240,
          "elapsed": 384055,
          "user": {
            "displayName": "Umar Ahsan",
            "photoUrl": "//lh3.googleusercontent.com/-KZJFBGO1WUE/AAAAAAAAAAI/AAAAAAAAAFk/G7WqmF0NCrk/s50-c-k-no/photo.jpg",
            "userId": "101835716642345366972"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import optimizers\n",
        "\n",
        "train_labels = to_categorical(trainlabel_encoded, num_classes=None)\n",
        "test_labels = to_categorical(testlabel_encoded, num_classes=None)\n",
        "numpy.random.seed(7)\n",
        "learning_rate=0.006 # Defining the learning rate\n",
        "epochs=30 # Defining the number of epochs for training\n",
        "decayrate= learning_rate/epochs\n",
        "sgd1= optimizers.Adam(lr=learning_rate, beta_1=0.65, beta_2=0.75, epsilon=1e-8, decay=0.07, amsgrad=False) # Adam optimizer is used for model training\n",
        "# # create the model\n",
        "embedding_vecor_length = 140\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(unique_pair), embedding_vecor_length, input_length=None))\n",
        "#model.add(LSTM(15,recurrent_activation='relu',unroll=True,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(140,recurrent_activation='relu',unroll=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
        "#print(model.summary())\n",
        "model.fit(traindata_encoded, train_labels,validation_data=(testdata_encoded, test_labels), nb_epoch=epochs, batch_size=100)\n",
        "# # Final evaluation of the model\n",
        "scores = model.evaluate(testdata_encoded, test_labels, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
            "Train on 6056 samples, validate on 1514 samples\n",
            "Epoch 1/30\n",
            "6056/6056 [==============================] - 14s 2ms/step - loss: 1.8635 - acc: 0.2498 - val_loss: 1.7789 - val_acc: 0.3236\n",
            "Epoch 2/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.6976 - acc: 0.3542 - val_loss: 1.7391 - val_acc: 0.3560\n",
            "Epoch 3/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.6185 - acc: 0.3932 - val_loss: 1.7019 - val_acc: 0.3454\n",
            "Epoch 4/30\n",
            "2300/6056 [==========>...................] - ETA: 7s - loss: 1.6098 - acc: 0.3778"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.5701 - acc: 0.4072 - val_loss: 1.6618 - val_acc: 0.3884\n",
            "Epoch 5/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.5208 - acc: 0.4346 - val_loss: 1.6247 - val_acc: 0.3937\n",
            "Epoch 6/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.4726 - acc: 0.4543 - val_loss: 1.6223 - val_acc: 0.3950\n",
            "Epoch 7/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.4463 - acc: 0.4647 - val_loss: 1.5924 - val_acc: 0.4135\n",
            "Epoch 8/30\n",
            "4800/6056 [======================>.......] - ETA: 2s - loss: 1.4081 - acc: 0.4798"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.4095 - acc: 0.4818 - val_loss: 1.5826 - val_acc: 0.4188\n",
            "Epoch 9/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.3815 - acc: 0.5018 - val_loss: 1.5890 - val_acc: 0.4227\n",
            "Epoch 10/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.3617 - acc: 0.5040 - val_loss: 1.5691 - val_acc: 0.4201\n",
            "Epoch 11/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.3382 - acc: 0.5175 - val_loss: 1.5479 - val_acc: 0.4267\n",
            "Epoch 12/30\n",
            "5400/6056 [=========================>....] - ETA: 1s - loss: 1.3237 - acc: 0.5150"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.3219 - acc: 0.5162 - val_loss: 1.5546 - val_acc: 0.4293\n",
            "Epoch 13/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.3009 - acc: 0.5246 - val_loss: 1.5547 - val_acc: 0.4313\n",
            "Epoch 14/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2905 - acc: 0.5325 - val_loss: 1.5368 - val_acc: 0.4346\n",
            "Epoch 15/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2703 - acc: 0.5343 - val_loss: 1.5356 - val_acc: 0.4445\n",
            "Epoch 16/30\n",
            "5200/6056 [========================>.....] - ETA: 1s - loss: 1.2554 - acc: 0.5379"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2541 - acc: 0.5401 - val_loss: 1.5219 - val_acc: 0.4399\n",
            "Epoch 17/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2308 - acc: 0.5560 - val_loss: 1.5215 - val_acc: 0.4538\n",
            "Epoch 18/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2371 - acc: 0.5507 - val_loss: 1.5148 - val_acc: 0.4538\n",
            "Epoch 19/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2120 - acc: 0.5575 - val_loss: 1.5029 - val_acc: 0.4531\n",
            "Epoch 20/30\n",
            "5400/6056 [=========================>....] - ETA: 1s - loss: 1.2050 - acc: 0.5644"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2058 - acc: 0.5662 - val_loss: 1.4996 - val_acc: 0.4551\n",
            "Epoch 21/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.2009 - acc: 0.5657 - val_loss: 1.5049 - val_acc: 0.4544\n",
            "Epoch 22/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1878 - acc: 0.5694 - val_loss: 1.4982 - val_acc: 0.4597\n",
            "Epoch 23/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1875 - acc: 0.5687 - val_loss: 1.4816 - val_acc: 0.4630\n",
            "Epoch 24/30\n",
            "5500/6056 [==========================>...] - ETA: 1s - loss: 1.1793 - acc: 0.5742"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1787 - acc: 0.5738 - val_loss: 1.4873 - val_acc: 0.4584\n",
            "Epoch 25/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1651 - acc: 0.5814 - val_loss: 1.4945 - val_acc: 0.4696\n",
            "Epoch 26/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1562 - acc: 0.5814 - val_loss: 1.4844 - val_acc: 0.4716\n",
            "Epoch 27/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1485 - acc: 0.5784 - val_loss: 1.4871 - val_acc: 0.4696\n",
            "Epoch 28/30\n",
            "4900/6056 [=======================>......] - ETA: 2s - loss: 1.1464 - acc: 0.5869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6056/6056 [==============================] - 13s 2ms/step - loss: 1.1412 - acc: 0.5888 - val_loss: 1.4833 - val_acc: 0.4749\n",
            "Epoch 29/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1424 - acc: 0.5845 - val_loss: 1.4828 - val_acc: 0.4828\n",
            "Epoch 30/30\n",
            "6056/6056 [==============================] - 12s 2ms/step - loss: 1.1456 - acc: 0.5859 - val_loss: 1.4735 - val_acc: 0.4782\n",
            "Accuracy: 47.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ArCtoRHc8kxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Further Analysis of Tranined Model\n",
        "\n",
        "Since 7 emotions are present in the dataset so we are curious to understand that which emotions are more correctly classified as compared to others. So, we further break down our accuracy into 7 different emotions accuracy"
      ]
    },
    {
      "metadata": {
        "id": "el6wcnxyDma1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6d983bb3-37ef-4f84-fdf8-27a867c86da9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527128854200,
          "user_tz": 240,
          "elapsed": 100212,
          "user": {
            "displayName": "Umar Ahsan",
            "photoUrl": "//lh3.googleusercontent.com/-KZJFBGO1WUE/AAAAAAAAAAI/AAAAAAAAAFk/G7WqmF0NCrk/s50-c-k-no/photo.jpg",
            "userId": "101835716642345366972"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Initializing true positive and true negative to calculate the accuracy of our model\n",
        "true_anger=0\n",
        "true_disgust=0\n",
        "true_fear=0\n",
        "true_guilt=0\n",
        "true_joy=0\n",
        "true_sadness=0\n",
        "true_shame=0\n",
        "false_anger = 0\n",
        "false_disgust = 0\n",
        "false_fear = 0\n",
        "false_guilt = 0\n",
        "false_joy = 0\n",
        "false_sadness = 0\n",
        "false_shame = 0\n",
        "\n",
        "for i in range(len(testdata_encoded)):\n",
        "  matrix = np.matrix(testdata_encoded[i])\n",
        "  matrix1 = np.matrix(testlabel_encoded[i])\n",
        "  result = model.predict(matrix, batch_size=None, verbose=0, steps=None)\n",
        "  if matrix1 == [0]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_anger = true_anger + 1\n",
        "    else:\n",
        "        false_anger = false_anger+1\n",
        "\n",
        "  if matrix1 == [1]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_disgust = true_disgust + 1\n",
        "    else:\n",
        "        false_disgust = false_disgust+1\n",
        "\n",
        "  if matrix1 == [2]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_fear = true_fear + 1\n",
        "    else:\n",
        "        false_fear = false_fear+1\n",
        "\n",
        "  if matrix1 == [3]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_guilt = true_guilt + 1\n",
        "    else:\n",
        "        false_guilt = false_guilt+1\n",
        "\n",
        "  if matrix1 == [4]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_joy = true_joy + 1\n",
        "    else:\n",
        "        false_joy = false_joy+1\n",
        "\n",
        "  if matrix1 == [5]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_sadness = true_sadness + 1\n",
        "    else:\n",
        "        false_sadness = false_sadness+1\n",
        "\n",
        "  if matrix1 == [6]:\n",
        "    if numpy.argmax(result)==matrix1:\n",
        "        true_shame = true_shame + 1\n",
        "    else:\n",
        "        false_shame = false_shame+1\n",
        "\n",
        "avg_anger = (true_anger / (true_anger+false_anger))\n",
        "avg_disgust = (true_disgust / (true_disgust + false_disgust))\n",
        "avg_fear = (true_fear / (true_fear + false_fear))\n",
        "avg_guilt = (true_guilt / (true_guilt + false_guilt))\n",
        "avg_joy = (true_joy / (true_joy + false_joy))\n",
        "avg_sadness =  (true_sadness / (true_sadness + false_sadness))\n",
        "avg_shame = (true_shame / (true_shame + false_shame))\n",
        "\n",
        "print(\"% Anger correct classify:\",avg_anger * 100)\n",
        "print(\"% Disgust correct classify:\",avg_disgust  * 100)\n",
        "print(\"% Fear correct classify:\",avg_fear  * 100)\n",
        "print(\"% Guilt correct classify:\",avg_guilt  * 100)\n",
        "print(\"% Joy correct classify:\",avg_joy  * 100)\n",
        "print(\"% Sadness correct classify:\",avg_sadness  * 100)\n",
        "print(\"% Shame correct classify:\",avg_shame  * 100)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% Anger correct classify: 45.851528384279476\n",
            "% Disgust correct classify: 54.82233502538071\n",
            "% Fear correct classify: 57.72727272727273\n",
            "% Guilt correct classify: 33.0188679245283\n",
            "% Joy correct classify: 62.616822429906534\n",
            "% Sadness correct classify: 45.410628019323674\n",
            "% Shame correct classify: 36.59574468085106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8bFje3I-9a6T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "We have used Deep Neural Network to classify emotions present within a sentence. It is further analyzed that not all of the emotions can be predicted accurately. Few of the emotions are easy to predict like \"Joy\" while other emotions like \"Shame\" are difficult to predict.\n",
        "\n",
        "\n",
        "# Future Work\n",
        "We are further expanding our work to improve the accuracy of our model using other modern tools like 'Attention'."
      ]
    }
  ]
}